{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yY2CN4uJju0C"
      ],
      "authorship_tag": "ABX9TyMs1BhEbqlhL/8vBuO168Af",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hazhyni/07-crypto-policy-x-review/blob/main/Crypto_Sentiment_Analysis_Pre_Post_Policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crypto Sentiment Analysis"
      ],
      "metadata": {
        "id": "tg3_HL4zjdCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Library"
      ],
      "metadata": {
        "id": "L9s-v-zsjje6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDznLpLf6yza"
      },
      "outputs": [],
      "source": [
        "!pip install snscrape pandas scikit-learn nltk sastrawi\n",
        "!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Node Package Installation\n"
      ],
      "metadata": {
        "id": "yY2CN4uJju0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Node.js (because tweet-harvest built using Node.js)\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ca-certificates curl gnupg\n",
        "!sudo mkdir -p /etc/apt/keyrings\n",
        "!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n",
        "\n",
        "!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n",
        "\n",
        "!sudo apt-get update\n",
        "\n",
        "!sudo apt-get install nodejs -y\n",
        "\n",
        "!node -v"
      ],
      "metadata": {
        "id": "DMjea9CPjyJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Policy"
      ],
      "metadata": {
        "id": "WO9eCJLjj8fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Crawl Data Pra(2009-2018)"
      ],
      "metadata": {
        "id": "Ni8cXlBukI5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"sentimen_kripto_2009_2018.csv\"\n",
        "search_keyword = \"kripto since:2009-01-01 until:2018-12-31 lang:in\"\n",
        "limit = 10000\n",
        "\n",
        "!npx --yes tweet-harvest@latest -o \"{filename}\" -s \"{search_keyword}\" -l \"{limit}\" --token \"f59f3fb97889932c58beba249dfab6b07eb81809\""
      ],
      "metadata": {
        "id": "pFQo3-93kIAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing App Review Pre-Policy"
      ],
      "metadata": {
        "id": "GaTvxEdSkUs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the path to your CSV file\n",
        "file_path_pra = f\"tweets-data/{filename}\"\n",
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path_pra, delimiter=\",\")\n",
        "\n",
        "df.rename(columns={\"full_text\": \"tweet\"}, inplace=True)\n",
        "selected_columns = df[[\"created_at\",\"tweet\", \"lang\"]]\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df.head(3))"
      ],
      "metadata": {
        "id": "b6g1AUolj6KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ],
      "metadata": {
        "id": "VUq12VYkkdra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Download the punkt_tab resource required for word_tokenize/sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the dataset (assuming df is already loaded as in the preceding code)\n",
        "# df = pd.read_csv('sentimen_kripto_2009_2019.csv', delimiter=',')\n",
        "\n",
        "# --- Cleaning ---\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # remove urls\n",
        "    text = re.sub(r'\\@\\w+', '', text) # remove mentions\n",
        "    text = re.sub(r'\\#\\w+', '', text) # remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    text = text.strip() # remove leading/trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # remove multiple spaces\n",
        "    return text\n",
        "\n",
        "df['cleaned_tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# --- Case Folding (already done in clean_text) ---\n",
        "\n",
        "# --- Tokenization ---\n",
        "df['tokenized_tweet'] = df['cleaned_tweet'].apply(lambda x: nltk.word_tokenize(x))\n",
        "\n",
        "# --- Stopword Removal ---\n",
        "list_stopwords = set(stopwords.words('indonesian'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in list_stopwords]\n",
        "\n",
        "df['stopwords_removed_tweet'] = df['tokenized_tweet'].apply(remove_stopwords)\n",
        "\n",
        "# --- Stemming ---\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_words(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Stemming after stopword removal\n",
        "df['stemmed_tweet'] = df['stopwords_removed_tweet'].apply(stem_words)\n",
        "\n",
        "# Display the progress (optional)\n",
        "print(\"\\nDataFrame after preprocessing:\")\n",
        "display(df[['tweet', 'cleaned_tweet', 'tokenized_tweet', 'stopwords_removed_tweet', 'stemmed_tweet']].head(5))"
      ],
      "metadata": {
        "id": "50yUIm3SkgyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Labelling"
      ],
      "metadata": {
        "id": "FlgIS94_kigQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Labelling Function with Sensitivity\n",
        "def manual_labeling_sensitive(text):\n",
        "\n",
        "    # Define keywords and phrases for each sentiment category (adjust these based on your specific data)\n",
        "    positive_keywords = [\n",
        "        \"naik\", \"untung\", \"profit\", \"bagus\", \"optimis\", \"sukses\", \"menguat\", \"bullish\",\n",
        "        \"peluang\", \"potensi\", \"menarik\", \"rekomendasi beli\", \"layak investasi\", \"akan naik\",\n",
        "        \"prospek cerah\", \"baik\", \"aman\", \"terpercaya\", \"menguntungkan\",\"keren\",\"positif\",\"beruntung\",\"bangun\",\"baik\",\"layak\",\"tinggi\",\"dipercaya\"\n",
        "    ]\n",
        "    negative_keywords = [\n",
        "        \"turun\", \"rugi\", \"loss\", \"buruk\", \"pesimis\", \"gagal\", \"melemah\", \"bearish\",\n",
        "        \"resiko\", \"bahaya\", \"rugi besar\", \"jual\", \"akan turun\", \"prospek suram\", \"jelek\",\n",
        "        \"tidak aman\", \"penipuan\", \"merugikan\", \"hati-hati\",\"jelek\",\"hacker\",\"retas\",\"menurun\",\"negatif\",\"volatilitas\",\"rendah\"\n",
        "    ]\n",
        "    neutral_keywords = [\n",
        "        \"harga\", \"pasar\", \"kripto\", \"bitcoin\", \"ethereum\", \"transaksi\",\n",
        "        \"berita\", \"analisis\", \"prediksi\", \"pergerakan\", \"update\", \"informasi\", \"data\",\n",
        "        \"volume\", \"kapitalisasi pasar\", \"blockchain\", \"wallet\", \"exchange\"\n",
        "    ]\n",
        "\n",
        "    # Simple check for negative sentiment first\n",
        "    for keyword in negative_keywords:\n",
        "        if keyword in text:\n",
        "            return 'Negative'\n",
        "\n",
        "    # Check for positive sentiment\n",
        "    for keyword in positive_keywords:\n",
        "        if keyword in text:\n",
        "            return 'Positive'\n",
        "\n",
        "    # If neither strong positive nor negative keywords are found, check for neutral\n",
        "    # This part is more sensitive to data differences. We check if any neutral keywords are present\n",
        "    # without significant positive or negative indicators.\n",
        "    for keyword in neutral_keywords:\n",
        "        if keyword in text:\n",
        "            # You could add more nuanced checks here if needed,\n",
        "            # e.g., checking for negation before positive/negative words,\n",
        "            # but for basic sensitivity, just the presence of neutral words\n",
        "            # after checking for strong sentiment can suffice for some cases.\n",
        "            return 'Neutral'\n",
        "\n",
        "    # If no matching keywords are found, label as Neutral by default\n",
        "    return 'Neutral'\n",
        "\n",
        "# Apply the manual labeling function to the processed text column\n",
        "# Choose the column that is most suitable for labeling after preprocessing.\n",
        "# 'stemmed_tweet' is usually a good candidate as it's been cleaned and stemmed.\n",
        "if 'stemmed_tweet' in df.columns:\n",
        "    df['label_result'] = df['stemmed_tweet'].apply(manual_labeling_sensitive)\n",
        "    print(\"Hasil Labelling dengan Lexicon Based Sentiment Analysis.\")\n",
        "    display(df[['tweet', 'stemmed_tweet', 'label_result']].head())\n",
        "else:\n",
        "    print(\"Column 'stemmed_tweet' not found for manual labeling. Please check your preprocessing steps.\")\n",
        "\n",
        "# You can now analyze the distribution of labels\n",
        "if 'label_result' in df.columns:\n",
        "    sentiment_counts = df['label_result'].value_counts()\n",
        "    print(\"\\nSentiment Label Distribution:\")\n",
        "sentiment_counts\n",
        "\n",
        "# You can also save the labeled data to a new CSV file\n",
        "# df.to_csv('dataset_kripto_labeled_manual.csv', index=False)\n",
        "# print(\"\\nLabeled data saved to 'dataset_kripto_labeled_manual.csv'\")\n"
      ],
      "metadata": {
        "id": "TWATZJVckodi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output"
      ],
      "metadata": {
        "id": "cluqaHi6ksL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: buatkan hasil visualisasi wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join all the stemmed tweets into a single string\n",
        "all_words = ' '.join([text for text in df['stemmed_tweet'].astype(str)])\n",
        "\n",
        "# Create the WordCloud object\n",
        "wordcloud = WordCloud(width = 800, height = 400,\n",
        "                background_color ='white',\n",
        "                stopwords = list_stopwords, # Use the stopwords list from preprocessing\n",
        "                min_font_size = 10).generate(all_words)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sdTjqXNWkuTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: buatkan analisis k-fold cross validationnya\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Prepare data for K-Fold Cross-Validation\n",
        "# We will use the 'stemmed_tweet' as the feature and 'label_result' as the target\n",
        "X = df['stemmed_tweet'].astype(str)  # Convert to string to handle potential non-string entries\n",
        "y = df['label_result']\n",
        "\n",
        "# Initialize KFold\n",
        "n_splits = 5  # You can change the number of folds\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Initialize the classifier (using Support Vector Machine as an example)\n",
        "model = SVC(kernel='linear')\n",
        "\n",
        "# Lists to store evaluation results\n",
        "accuracy_scores = []\n",
        "classification_reports = []\n",
        "\n",
        "print(f\"Performing {n_splits}-Fold Cross-Validation...\")\n",
        "\n",
        "# Perform K-Fold Cross-Validation\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "    # Transform the testing data\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    # Evaluate the model\n",
        "    fold_accuracy = accuracy_score(y_test, y_pred)\n",
        "    fold_report = classification_report(y_test, y_pred, zero_division=0) # zero_division=0 handles cases where a class has no predicted samples\n",
        "\n",
        "    accuracy_scores.append(fold_accuracy)\n",
        "    classification_reports.append(fold_report)\n",
        "\n",
        "    print(f\"Fold {fold + 1} Accuracy: {fold_accuracy:.4f}\")\n",
        "    print(f\"Fold {fold + 1} Classification Report:\\n{fold_report}\")\n",
        "\n",
        "# Print overall results\n",
        "print(\"\\n--- Overall K-Fold Cross-Validation Results ---\")\n",
        "print(f\"Average Accuracy across {n_splits} folds: {sum(accuracy_scores)/n_splits:.4f}\")\n",
        "\n",
        "# You can also print the report for each fold or the report from the last fold\n",
        "# print(\"\\nClassification Report from the last fold:\")\n",
        "# print(classification_reports[-1])\n",
        "\n",
        "# To get an aggregate report across all folds, you would typically combine predictions\n",
        "# or average metrics in a more complex way, but the average accuracy gives a good overall idea.\n"
      ],
      "metadata": {
        "id": "v-apCOd7kxwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pie chart in grayscale\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=['#808080', '#A9A9A9', '#D3D3D3'], startangle=140)\n",
        "plt.title('Hasil Analisis Sentimen', fontsize=16)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DqxS7xUGkz1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pasca-Policy"
      ],
      "metadata": {
        "id": "VAQ3JLSIn20b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Crawl Data"
      ],
      "metadata": {
        "id": "nVIIS9byoAiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"sentimen_kripto_2019_2025.csv\"\n",
        "search_keyword = \"kripto since:2019-01-01 until:2025-05-31 lang:in,id\"\n",
        "limit = 10000\n",
        "\n",
        "!npx --yes tweet-harvest@latest -o \"{filename}\" -s \"{search_keyword}\" -l \"{limit}\" --token \"f59f3fb97889932c58beba249dfab6b07eb81809\""
      ],
      "metadata": {
        "id": "JU8ROI4en5yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "NahQoVfOoCEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your CSV file\n",
        "file_path_pra = f\"tweets-data/{filename}\"\n",
        "\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path_pra, delimiter=\",\")\n",
        "\n",
        "df.rename(columns={\"full_text\": \"tweet\"}, inplace=True)\n",
        "selected_columns = df[[\"created_at\",\"tweet\", \"lang\"]]\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df.head(3))"
      ],
      "metadata": {
        "id": "nmGyd0p9n9oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek jumlah data yang didapatkan\n",
        "\n",
        "num_tweets = len(df)\n",
        "print(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")"
      ],
      "metadata": {
        "id": "lIJi56qsoICJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Download the punkt_tab resource required for word_tokenize/sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load the dataset (assuming df is already loaded as in the preceding code)\n",
        "# df = pd.read_csv('sentimen_kripto_2009_2019.csv', delimiter=',')\n",
        "\n",
        "# --- Cleaning ---\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # remove urls\n",
        "    text = re.sub(r'\\@\\w+', '', text) # remove mentions\n",
        "    text = re.sub(r'\\#\\w+', '', text) # remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    text = text.strip() # remove leading/trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # remove multiple spaces\n",
        "    return text\n",
        "\n",
        "df['cleaned_tweet'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# --- Case Folding (already done in clean_text) ---\n",
        "\n",
        "# --- Tokenization ---\n",
        "df['tokenized_tweet'] = df['cleaned_tweet'].apply(lambda x: nltk.word_tokenize(x))\n",
        "\n",
        "# --- Stopword Removal ---\n",
        "list_stopwords = set(stopwords.words('indonesian'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in list_stopwords]\n",
        "\n",
        "df['stopwords_removed_tweet'] = df['tokenized_tweet'].apply(remove_stopwords)\n",
        "\n",
        "# --- Stemming ---\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_words(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Stemming after stopword removal\n",
        "df['stemmed_tweet'] = df['stopwords_removed_tweet'].apply(stem_words)\n",
        "\n",
        "# Display the progress (optional)\n",
        "print(\"\\nDataFrame after preprocessing:\")\n",
        "display(df[['tweet', 'cleaned_tweet', 'tokenized_tweet', 'stopwords_removed_tweet', 'stemmed_tweet']].head(5))"
      ],
      "metadata": {
        "id": "5fadP8dXoJcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentiment Classification"
      ],
      "metadata": {
        "id": "_qHIZiPwoFl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Labelling Function with Sensitivity\n",
        "def manual_labeling_sensitive(text):\n",
        "\n",
        "    # Define keywords and phrases for each sentiment category (adjust these based on your specific data)\n",
        "    positive_keywords = [\n",
        "        \"naik\", \"untung\", \"profit\", \"bagus\", \"optimis\", \"sukses\", \"menguat\", \"bullish\",\n",
        "        \"peluang\", \"potensi\", \"menarik\", \"rekomendasi beli\", \"layak investasi\", \"akan naik\",\n",
        "        \"prospek cerah\", \"baik\", \"aman\", \"terpercaya\", \"menguntungkan\",\"keren\",\"positif\",\"beruntung\",\"bangun\",\"baik\",\"layak\",\"tinggi\",\"dipercaya\"\n",
        "    ]\n",
        "    negative_keywords = [\n",
        "        \"turun\", \"rugi\", \"loss\", \"buruk\", \"pesimis\", \"gagal\", \"melemah\", \"bearish\",\n",
        "        \"resiko\", \"bahaya\", \"rugi besar\", \"jual\", \"akan turun\", \"prospek suram\", \"jelek\",\n",
        "        \"tidak aman\", \"penipuan\", \"merugikan\", \"hati-hati\",\"jelek\",\"hacker\",\"retas\",\"menurun\",\"negatif\",\"volatilitas\",\"rendah\"\n",
        "    ]\n",
        "    neutral_keywords = [\n",
        "        \"harga\", \"pasar\", \"kripto\", \"bitcoin\", \"ethereum\", \"transaksi\",\n",
        "        \"berita\", \"analisis\", \"prediksi\", \"pergerakan\", \"update\", \"informasi\", \"data\",\n",
        "        \"volume\", \"kapitalisasi pasar\", \"blockchain\", \"wallet\", \"exchange\"\n",
        "    ]\n",
        "\n",
        "    # Simple check for negative sentiment first\n",
        "    for keyword in negative_keywords:\n",
        "        if keyword in text:\n",
        "            return 'Negative'\n",
        "\n",
        "    # Check for positive sentiment\n",
        "    for keyword in positive_keywords:\n",
        "        if keyword in text:\n",
        "            return 'Positive'\n",
        "\n",
        "    # If neither strong positive nor negative keywords are found, check for neutral\n",
        "    # This part is more sensitive to data differences. We check if any neutral keywords are present\n",
        "    # without significant positive or negative indicators.\n",
        "    for keyword in neutral_keywords:\n",
        "        if keyword in text:\n",
        "            # You could add more nuanced checks here if needed,\n",
        "            # e.g., checking for negation before positive/negative words,\n",
        "            # but for basic sensitivity, just the presence of neutral words\n",
        "            # after checking for strong sentiment can suffice for some cases.\n",
        "            return 'Neutral'\n",
        "\n",
        "    # If no matching keywords are found, label as Neutral by default\n",
        "    return 'Neutral'\n",
        "\n",
        "# Apply the manual labeling function to the processed text column\n",
        "# Choose the column that is most suitable for labeling after preprocessing.\n",
        "# 'stemmed_tweet' is usually a good candidate as it's been cleaned and stemmed.\n",
        "if 'stemmed_tweet' in df.columns:\n",
        "    df['label_result'] = df['stemmed_tweet'].apply(manual_labeling_sensitive)\n",
        "    print(\"Hasil Labelling dengan Lexicon Based Sentiment Analysis.\")\n",
        "    display(df[['tweet', 'stemmed_tweet', 'label_result']].head())\n",
        "else:\n",
        "    print(\"Column 'stemmed_tweet' not found for manual labeling. Please check your preprocessing steps.\")\n",
        "\n",
        "# You can now analyze the distribution of labels\n",
        "if 'label_result' in df.columns:\n",
        "    sentiment_counts = df['label_result'].value_counts()\n",
        "    print(\"\\nSentiment Label Distribution:\")\n",
        "sentiment_counts\n",
        "\n",
        "# You can also save the labeled data to a new CSV file\n",
        "# df.to_csv('dataset_kripto_labeled_manual.csv', index=False)\n",
        "# print(\"\\nLabeled data saved to 'dataset_kripto_labeled_manual.csv'\")\n"
      ],
      "metadata": {
        "id": "uEWxD2UJoNKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Output"
      ],
      "metadata": {
        "id": "hxQlLOT_oP4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: buatkan hasil visualisasi wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Join all the stemmed tweets into a single string\n",
        "all_words = ' '.join([text for text in df['stemmed_tweet'].astype(str)])\n",
        "\n",
        "# Create the WordCloud object\n",
        "wordcloud = WordCloud(width = 800, height = 400,\n",
        "                background_color ='white',\n",
        "                stopwords = list_stopwords, # Use the stopwords list from preprocessing\n",
        "                min_font_size = 10).generate(all_words)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Er_YdzHzoUGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pie chart in grayscale\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=['#808080', '#A9A9A9', '#D3D3D3'], startangle=140)\n",
        "plt.title('Hasil Analisis Sentimen', fontsize=16)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xJor4EY1oV_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}